{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inporting our own functions\n",
    "from modules.TrainTestValGen import init_gens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, Activation, Dropout, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TF memory handling \n",
    "# # TensorFlow wizardry\n",
    "# config = tf.ConfigProto()\n",
    " \n",
    "# # Don't pre-allocate memory; allocate as-needed\n",
    "# config.gpu_options.allow_growth = True\n",
    " \n",
    "# # Only allow a total of half the GPU memory to be allocated\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# # Create a session with the above options specified.\n",
    "# k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "BBox_df = pd.read_csv('../BBox_List_2017.csv')\n",
    "de_df = pd.read_csv('../Data_Entry_2017.csv')\n",
    "train_filter = pd.read_csv('../train_val_list.txt')\n",
    "train_filter.columns=['Image_Index']\n",
    "test_filter = pd.read_csv('../test_list.txt')\n",
    "test_filter.columns=['Image_Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df =  86511  test_df =  25591\n"
     ]
    }
   ],
   "source": [
    "# clean up incorrect ages in the data\n",
    "bad_ages = de_df[de_df['Patient Age'] >= 100].index\n",
    "\n",
    "df = de_df.drop(bad_ages)\n",
    "\n",
    "df.drop(columns=['Unnamed: 11','OriginalImagePixelSpacing[x','y]','OriginalImage[Width','Height]'], inplace=True)\n",
    "\n",
    "df.columns = ['Image_Index', 'Finding_Labels', 'Follow-up', 'Patient ID',\n",
    "       'Patient_Age', 'Patient_Gender', 'View_Position']\n",
    "\n",
    "# Remove secondary labels\n",
    "df['Finding_Labels'] = df['Finding_Labels'].apply(lambda x: x.split('|')[0])\n",
    "\n",
    "\n",
    "# Join to the list of test and training to split the data set\n",
    "train_df = pd.merge(train_filter,df , on='Image_Index', how='inner')\n",
    "test_df = pd.merge(test_filter,df , on='Image_Index', how='inner')\n",
    "\n",
    "print(f'train_df =  {len(train_df)}  test_df =  {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size Images \n",
    "datagen = ImageDataGenerator(rescale=.05,validation_split=0.25)\n",
    "test_datagen=ImageDataGenerator(rescale=.05)\n",
    "# How we bin out the batches to run onto the GPU\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _thread import start_new_thread\n",
    "\n",
    "#Attach CSV to Images Folder and defline our traiing validation and test\n",
    "def vaild_gen(batch_fit):\n",
    "    global valid_generator\n",
    "    valid_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory=\"../images/\",\n",
    "        x_col=\"Image_Index\",\n",
    "        y_col=\"Finding_Labels\",\n",
    "        subset=\"validation\",\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(15,15),\n",
    "        batch_size=batch_fit)\n",
    "    \n",
    "def train_gen(batch_fit):\n",
    "    global train_generator\n",
    "    train_generator=datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory=\"../images/\",\n",
    "        x_col=\"Image_Index\",\n",
    "        y_col=\"Finding_Labels\",\n",
    "        subset=\"training\",\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(15,15),\n",
    "        batch_size=batch_fit)\n",
    "    \n",
    "def test_gen(batch_fit):\n",
    "    global test_generator\n",
    "    test_generator=test_datagen.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        directory=\"../images/\",\n",
    "        x_col=\"Image_Index\",\n",
    "        y_col=\"Finding_Labels\",\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(15,15),\n",
    "        batch_size=batch_fit)\n",
    "    \n",
    "def init_gens(batch_fit):\n",
    "    start_new_thread(vaild_gen,(batch_fit, ))\n",
    "    start_new_thread(train_gen,(batch_fit ,))\n",
    "    start_new_thread(test_gen,(batch_fit, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test 20 train 20 val 20 \n",
      "Found 23438 validated image filenames belonging to 15 classes.\n",
      "Found 59001 validated image filenames belonging to 15 classes.Found 19666 validated image filenames belonging to 15 classes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init_gens(BATCH_SIZE)\n",
    "\n",
    "print(f' test {test_generator.batch_size} train {train_generator.batch_size} val {valid_generator.batch_size} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(15, (3, 3), padding='same',\n",
    "                 input_shape=(15,15,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(15, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(15, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 86512)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test Shape of data\n",
    "train_labels = to_categorical(train_df.shape)\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n",
      "Exception in thread Thread-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 666, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 661, in <lambda>\n",
      "    initargs=(seqs, self.random_seed))\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\context.py\", line 119, in Pool\n",
      "    context=self.get_context())\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\pool.py\", line 176, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\pool.py\", line 241, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\process.py\", line 112, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\context.py\", line 322, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\popen_spawn_win32.py\", line 89, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "TypeError: can't pickle _thread.lock objects\n",
      "\n",
      "Exception in thread Thread-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 666, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 661, in <lambda>\n",
      "    initargs=(seqs, self.random_seed))\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\context.py\", line 119, in Pool\n",
      "    context=self.get_context())\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\pool.py\", line 176, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\pool.py\", line 241, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\process.py\", line 112, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\context.py\", line 322, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\popen_spawn_win32.py\", line 89, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "TypeError: can't pickle _thread.lock objects\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "STEP_SIZE_VALID= valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "STEP_SIZE_TRAIN= train_generator.n//train_generator.batch_size\n",
    "\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "\n",
    "\n",
    "model.compile(optimizers.rmsprop(lr=0.0001),\n",
    "loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    workers=16,\n",
    "                    use_multiprocessing=True,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting global Memory https://github.com/keras-team/keras/issues/12625\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del classifier # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
