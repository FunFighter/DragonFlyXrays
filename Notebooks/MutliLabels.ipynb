{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inporting our own functions\n",
    "from modules.TrainTestValGen import init_gens\n",
    "# @TODO add more to the modules folder to save space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from _thread import start_new_thread\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, optimizers\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, Activation, Dropout, MaxPooling2D, BatchNormalization\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import scipy.misc\n",
    "from scipy.stats import itemfreq\n",
    "from random import sample\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Image manipulation\n",
    "import PIL.Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "BBox_df = pd.read_csv('../BBox_List_2017.csv',dtype=str)\n",
    "de_df = pd.read_csv('../Data_Entry_2017.csv',dtype=str)\n",
    "train_filter = pd.read_csv('../train_val_list.txt',dtype=str)\n",
    "train_filter.columns=['Image_Index']\n",
    "test_filter = pd.read_csv('../test_list.txt',dtype=str)\n",
    "test_filter.columns=['Image_Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Index</th>\n",
       "      <th>Patient_Age</th>\n",
       "      <th>Patient_Gender</th>\n",
       "      <th>View_Position</th>\n",
       "      <th>Test_Split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000001_000.png</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>[Cardiomegaly]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000001_001.png</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>[Cardiomegaly, Emphysema]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000001_002.png</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>[Cardiomegaly, Effusion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000002_000.png</td>\n",
       "      <td>81</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>[No Finding]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000003_000.png</td>\n",
       "      <td>81</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>[Hernia]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Image_Index Patient_Age Patient_Gender View_Position  \\\n",
       "0  00000001_000.png          58              M            PA   \n",
       "1  00000001_001.png          58              M            PA   \n",
       "2  00000001_002.png          58              M            PA   \n",
       "3  00000002_000.png          81              M            PA   \n",
       "4  00000003_000.png          81              F            PA   \n",
       "\n",
       "                  Test_Split  \n",
       "0             [Cardiomegaly]  \n",
       "1  [Cardiomegaly, Emphysema]  \n",
       "2   [Cardiomegaly, Effusion]  \n",
       "3               [No Finding]  \n",
       "4                   [Hernia]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean up incorrect ages in the data\n",
    "\n",
    "bad_ages = de_df[de_df['Patient Age'].astype(int) >= 100].index\n",
    "\n",
    "df = de_df.drop(bad_ages)\n",
    "\n",
    "df.drop(columns=['Unnamed: 11','OriginalImagePixelSpacing[x','y]','OriginalImage[Width','Height]'], inplace=True)\n",
    "\n",
    "df.columns = ['Image_Index', 'Finding_Labels', 'Follow-up', 'Patient ID',\n",
    "       'Patient_Age', 'Patient_Gender', 'View_Position']\n",
    "\n",
    "# Remove secondary labels\n",
    "# @TODO handle labels better\n",
    "# df['Finding_Labels'] = df['Finding_Labels'].apply(lambda x: x.split('|')[0])\n",
    "\n",
    "#MAYBE TRY TO PASS A LIST OF INTS FOR THE LABELS IN ONE COLUMN\n",
    "\n",
    "df['Test_Split'] = df['Finding_Labels'].str.split('|')\n",
    "# df[['label1','label2','label3','label4','label5','label6','label7','label8','label9']] = pd.DataFrame(df['Test_Split'].values.tolist(),index=df.index)\n",
    "df.drop(columns=['Finding_Labels','Follow-up','Patient ID'], inplace=True)\n",
    "\n",
    "# Second one didn't work but worked before?\n",
    "df.replace(np.nan, 'No Finding', inplace=True)\n",
    "# df.replace('None','No Finding').head()\n",
    "\n",
    "\n",
    "# Join to the list of test and training to split the data set\n",
    "train_df = pd.merge(train_filter,df , on='Image_Index', how='inner')\n",
    "test_df = pd.merge(test_filter,df , on='Image_Index', how='inner')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112104, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Image_Index', 'Patient_Age', 'Patient_Gender', 'View_Position',\n",
       "       'Test_Split'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['label1',\n",
    " 'label2',\n",
    " 'label3',\n",
    " 'label4',\n",
    " 'label5',\n",
    " 'label6',\n",
    " 'label7',\n",
    " 'label8',\n",
    " 'label9']\n",
    "# labels.reshape(labels.shape[0],1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels=['Patient_Age', 'Patient_Gender', 'View_Position',\n",
    "       'label1', 'label2', 'label3', 'label4', 'label5', 'label6', 'label7',\n",
    "       'label8', 'label9']\n",
    "# y_labels=['Patient_Age','Patient_Gender','View_Position','Test_Split']\n",
    "# y_labels='Test_Split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size Images \n",
    "# @TODO test to see what the pixle size is \n",
    "datagen = ImageDataGenerator(rescale=1./255 ,validation_split=0.25)\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Class Mode\n",
    "Class_Value = 'categorical'\n",
    "\n",
    "# Target_size for all\n",
    "TARGET_SIZE = 270\n",
    "\n",
    "# How we bin out the batches to run onto the GPU\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO Get this working an import\n",
    "\n",
    "#Attach CSV to Images Folder and defline our traiing validation and test\n",
    "def vaild_gen(batch_fit, Class_Value, TARGET_SIZE):\n",
    "    global valid_generator\n",
    "    valid_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory=\"../images/\",\n",
    "        x_col=\"Image_Index\",\n",
    "        y_col='Test_Split',\n",
    "        subset=\"validation\",\n",
    "        class_mode=Class_Value,\n",
    "        target_size=(TARGET_SIZE,TARGET_SIZE),\n",
    "        batch_size=batch_fit)\n",
    "    \n",
    "def train_gen(batch_fit, Class_Value, TARGET_SIZE):\n",
    "    global train_generator\n",
    "    train_generator=datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory=\"../images/\",\n",
    "        x_col=\"Image_Index\",\n",
    "        y_col='Test_Split',\n",
    "        subset=\"training\",\n",
    "        class_mode=Class_Value,\n",
    "        target_size=(TARGET_SIZE,TARGET_SIZE),\n",
    "        batch_size=batch_fit)\n",
    "    \n",
    "def test_gen(batch_fit, Class_Value, TARGET_SIZE):\n",
    "    global test_generator\n",
    "    test_generator=test_datagen.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        directory=\"../images/\",\n",
    "        x_col=\"Image_Index\",\n",
    "        y_col='Test_Split',\n",
    "        class_mode=Class_Value,\n",
    "        target_size=(TARGET_SIZE,TARGET_SIZE),\n",
    "        batch_size=batch_fit)\n",
    "    \n",
    "def init_gens(batch_fit, class_value, target):\n",
    "    start_new_thread(vaild_gen,(batch_fit, class_value,target,  ))\n",
    "    start_new_thread(train_gen,(batch_fit, class_value,target, ))\n",
    "    start_new_thread(test_gen,(batch_fit, class_value,target, ))\n",
    "    \n",
    "# Resetting global Memory https://github.com/keras-team/keras/issues/12625\n",
    "# TODO move to module\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del classifier # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "    # use the same config as you used to create the session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tf.Session(config=config))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_gens(BATCH_SIZE ,Class_Value, TARGET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  test_generator=test_datagen.flow_from_dataframe(\n",
    "#         dataframe=test_df,\n",
    "#         directory=\"../images/\",\n",
    "#         x_col=\"Image_Index\",\n",
    "#         y_col=y_labels,\n",
    "#         class_mode=None,\n",
    "#         target_size=(128,128),\n",
    "#         batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO read https://medium.com/nanonets/how-to-easily-build-a-dog-breed-image-classification-model-2fd214419cde\n",
    "# and build a better CNN\n",
    "#layers\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), padding='same',\n",
    "#                  input_shape=(512,512,2)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Conv2D(128, (3,3)))\n",
    "\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(512))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(TARGET_SIZE,TARGET_SIZE,3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(15, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0801 05:40:51.398927 17596 deprecation_wrapper.py:119] From C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0801 05:40:51.402928 17596 deprecation_wrapper.py:119] From C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0801 05:40:51.451786 17596 deprecation.py:323] From C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "5900/5900 [==============================] - 778s 132ms/step - loss: 2.2963 - acc: 0.5657 - val_loss: 2.0323 - val_acc: 0.6259\n",
      "Epoch 2/25\n",
      "5900/5900 [==============================] - 773s 131ms/step - loss: 2.2600 - acc: 0.5657 - val_loss: 2.0525 - val_acc: 0.6258\n",
      "Epoch 3/25\n",
      "5900/5900 [==============================] - 774s 131ms/step - loss: 2.2615 - acc: 0.5657 - val_loss: 2.0443 - val_acc: 0.6251\n",
      "Epoch 4/25\n",
      "5900/5900 [==============================] - 767s 130ms/step - loss: 2.2626 - acc: 0.5658 - val_loss: 2.0728 - val_acc: 0.6256\n",
      "Epoch 5/25\n",
      "5900/5900 [==============================] - 769s 130ms/step - loss: 2.2724 - acc: 0.5664 - val_loss: 2.0704 - val_acc: 0.6256\n",
      "Epoch 6/25\n",
      "5900/5900 [==============================] - 779s 132ms/step - loss: 2.2963 - acc: 0.5663 - val_loss: 2.0828 - val_acc: 0.6261\n",
      "Epoch 7/25\n",
      "5900/5900 [==============================] - 821s 139ms/step - loss: 2.2994 - acc: 0.5666 - val_loss: 2.0742 - val_acc: 0.6262\n",
      "Epoch 8/25\n",
      "5900/5900 [==============================] - 814s 138ms/step - loss: 2.3045 - acc: 0.5663 - val_loss: 2.1088 - val_acc: 0.6251\n",
      "Epoch 9/25\n",
      "5900/5900 [==============================] - 817s 138ms/step - loss: 2.3079 - acc: 0.5667 - val_loss: 2.0746 - val_acc: 0.6258\n",
      "Epoch 10/25\n",
      "5900/5900 [==============================] - 819s 139ms/step - loss: 2.3095 - acc: 0.5661 - val_loss: 2.0664 - val_acc: 0.6261\n",
      "Epoch 11/25\n",
      "5900/5900 [==============================] - 808s 137ms/step - loss: 2.3426 - acc: 0.5665 - val_loss: 2.0791 - val_acc: 0.6254\n",
      "Epoch 12/25\n",
      "5900/5900 [==============================] - 790s 134ms/step - loss: 9.9586 - acc: 0.5663 - val_loss: 8.8143 - val_acc: 0.6257\n",
      "Epoch 13/25\n",
      "5900/5900 [==============================] - 779s 132ms/step - loss: 10.5773 - acc: 0.5666 - val_loss: 8.7979 - val_acc: 0.6258\n",
      "Epoch 14/25\n",
      "5900/5900 [==============================] - 779s 132ms/step - loss: 10.5776 - acc: 0.5665 - val_loss: 8.8094 - val_acc: 0.6255\n",
      "Epoch 15/25\n",
      "5900/5900 [==============================] - 779s 132ms/step - loss: 10.5800 - acc: 0.5665 - val_loss: 8.8044 - val_acc: 0.6260\n",
      "Epoch 16/25\n",
      "5900/5900 [==============================] - 779s 132ms/step - loss: 10.5811 - acc: 0.5663 - val_loss: 8.7897 - val_acc: 0.6263\n",
      "Epoch 17/25\n",
      "5900/5900 [==============================] - 780s 132ms/step - loss: 10.5811 - acc: 0.5663 - val_loss: 8.8069 - val_acc: 0.6255\n",
      "Epoch 18/25\n",
      "5900/5900 [==============================] - 779s 132ms/step - loss: 10.5751 - acc: 0.5663 - val_loss: 8.8126 - val_acc: 0.6255\n",
      "Epoch 19/25\n",
      "5900/5900 [==============================] - 779s 132ms/step - loss: 10.5869 - acc: 0.5663 - val_loss: 8.7954 - val_acc: 0.6262\n",
      "Epoch 20/25\n",
      "5900/5900 [==============================] - 779s 132ms/step - loss: 10.5710 - acc: 0.5666 - val_loss: 8.8249 - val_acc: 0.6249\n",
      "Epoch 21/25\n",
      "5900/5900 [==============================] - 780s 132ms/step - loss: 10.5847 - acc: 0.5663 - val_loss: 8.7979 - val_acc: 0.6260\n",
      "Epoch 22/25\n",
      "5900/5900 [==============================] - 761s 129ms/step - loss: 10.5808 - acc: 0.5664 - val_loss: 8.7930 - val_acc: 0.6262\n",
      "Epoch 23/25\n",
      "5900/5900 [==============================] - 734s 124ms/step - loss: 10.5677 - acc: 0.5667 - val_loss: 8.8085 - val_acc: 0.6260\n",
      "Epoch 24/25\n",
      "5900/5900 [==============================] - 734s 124ms/step - loss: 10.5945 - acc: 0.5659 - val_loss: 8.8094 - val_acc: 0.6259\n",
      "Epoch 25/25\n",
      "5900/5900 [==============================] - 734s 124ms/step - loss: 10.5770 - acc: 0.5665 - val_loss: 8.8102 - val_acc: 0.6252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x227c8d1e4a8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STEP_SIZE_VALID= valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "STEP_SIZE_TRAIN= train_generator.n//train_generator.batch_size\n",
    "\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "\n",
    "\n",
    "model.compile(optimizers.rmsprop(lr=0.0001),\n",
    "loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    workers=16,\n",
    "                    use_multiprocessing=False,\n",
    "                    epochs=25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py:273: UserWarning: Found 2153 invalid image filename(s) in x_col=\"Image_Index\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23438 validated image filenames belonging to 15 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saber\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py:273: UserWarning: Found 7844 invalid image filename(s) in x_col=\"Image_Index\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19666 validated image filenames belonging to 15 classes.\n",
      "Found 59001 validated image filenames belonging to 15 classes.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[574592,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_1/RMSprop/gradients/dense_3/MatMul_grad/MatMul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c32c7c393267>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m                     \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                     epochs=25) \n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[574592,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_1/RMSprop/gradients/dense_3/MatMul_grad/MatMul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "STEP_SIZE_VALID= valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "STEP_SIZE_TRAIN= train_generator.n//train_generator.batch_size\n",
    "\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "\n",
    "\n",
    "model.compile(optimizers.rmsprop(lr=0.0001),\n",
    "loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    workers=16,\n",
    "                    use_multiprocessing=False,\n",
    "                    epochs=25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
